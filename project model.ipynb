{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944764d5",
   "metadata": {},
   "source": [
    "# Predicting the Risk of Employees’ Attrition with Classification Models\n",
    "\n",
    "By: Evelyn Peh Ting Yu, Koh Pei Ling, Song Yu Xiang, Wong Jie Peng, Guo Peng Yuan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e021bd4",
   "metadata": {},
   "source": [
    "The US department of labour mentioned that “it could cost up to 11k in direct training expenses and lost productivity to replace an experienced employee earning an annual salary of $33,000.” This statement itself implies that employee attrition is costly. When an employee leaves the organization, the company is not only losing its valuable employees, but the company also loses on the amount that it has spent to recruit and select those employees and to train them for their respective jobs. On the other hand, the organization needs to invest more and more in recruitment, training, and development of new staff to fill up their vacant positions. Due to these reasons, every organization wants to control the attrition rate and retain its employees through more satisfactory company policies and work environments. \n",
    "\n",
    "This project, therefore, aims to predict the likelihood of an employee leaving the company by understanding the main drivers of employee churn. Such information can be valuable in helping management take possible actions to potentially retain these employees thereby reducing the high cost incurred in employee turnover. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bc8f3",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc853a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#model\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier # pip install xgboost\n",
    "\n",
    "from imblearn.over_sampling import SMOTE # !pip install imblearn\n",
    "\n",
    "#PyTorch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "#Save and Load Model\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463aa15",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b6ec2",
   "metadata": {},
   "source": [
    "### 2. Import Dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset\n",
    "\n",
    "<b>35 attributes, 1460 observations</b> <br><br>\n",
    "<b>Attributes:</b> <br>\n",
    "<b>Dependent Variables:</b> Attrition <br>\n",
    "<b>Independent Variables:</b>\n",
    "<ul>\n",
    "    <li><b><i>Basic Information (6 attributes)</i></b>: Age, Education, Education Field, Gender, Marital Status, Distance From Home\n",
    "    <li><b><i>Work Information (8 attributes)</i></b>: Department, Job Role, Job Level, Over Time, Business Travel, Performance Rating, Stock Option Level,\n",
    "          Job Involvement\n",
    "    <li><b><i>Satisfaction (4 attributes)</i></b>: Work-Life Balance, Job Satisfaction, Relationship Satisfaction, Enviroment Satisfaction\n",
    "    <li><b><i>Salary Related (5 attributes)</i></b>: Monthly Income, Monthly Rate, Daily Rate, Hourly Rate, Percent Salary Hike\n",
    "    <li><b><i>Time Related (7 attributes)</i></b>: Total Working Years, Training Time Last Year, Years At Company, Years In Current Role, Years Since Last Promotion, \n",
    "           Years With Current Manager, Num Companies Worked\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ca9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\") \n",
    "df.shape #1470 rows and 35 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc841dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head() #top 5 records, ensure data loaded properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62009e55",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b81f93",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230a6c7",
   "metadata": {},
   "source": [
    "refer to \"Project - Exploratory Data Analysis\" Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c22ae3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b967b1",
   "metadata": {},
   "source": [
    "### 4. Feature Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882d992",
   "metadata": {},
   "source": [
    "###### Dropping columns with constant value and identifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeNumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70af3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=col_to_drop, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #35 - 4 = 31 remaining cols "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab74c3",
   "metadata": {},
   "source": [
    "###### Encoding the categorical type data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb69946",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding will be used for columns with 2 or less unique values\n",
    "le_count = 0\n",
    "for col in df.columns[1:]:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            le.fit(df[col])\n",
    "            df[col] = le.transform(df[col])\n",
    "            le_count += 1\n",
    "print('{} columns were label encoded.'.format(le_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d821e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rest of categorical variable into dummy\n",
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13150e",
   "metadata": {},
   "source": [
    "###### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbd470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "HR_col = list(df.columns)\n",
    "HR_col.remove('Attrition')\n",
    "for col in HR_col:\n",
    "    df[col] = df[col].astype(float)\n",
    "    df[[col]] = scaler.fit_transform(df[[col]])\n",
    "df['Attrition'] = pd.to_numeric(df['Attrition'], downcast='float')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "y = df[\"Attrition\"]\n",
    "X = df.loc[:, df.columns != 'Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d565f5c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f57c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "original_X_train, original_X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd85bdc",
   "metadata": {},
   "source": [
    "### 5. Handling imbalance data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80f1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state = 0)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c79ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9e312",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718f4fc",
   "metadata": {},
   "source": [
    "### 6. Handling imbalance data with GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac5b89",
   "metadata": {},
   "source": [
    "https://realpython.com/generative-adversarial-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8999f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_gantrain, X_gantest, y_gantrain, y_gantest = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = X_gantrain.copy()\n",
    "process['target'] = y_gantrain.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba97df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be54419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_forgenerate = process.query(\"target == 1\").iloc[:,:-1].values\n",
    "X_forgenerate = torch.tensor(X_forgenerate).type(torch.FloatTensor)\n",
    "\n",
    "X_non_attrition = process.query('target == 0').iloc[:,:-1].values #without target col\n",
    "n_generate = X_non_attrition.shape[0] - X_forgenerate.shape[0]\n",
    "\n",
    "print(f'Have to generate {n_generate} record of attrition = 1 to balance data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "implmentation of generator\n",
    "it’s a model with a two-dimensional input, which will receive random points (z₁, z₂), \n",
    "and a two-dimensional output that must provide (x̃₁, x̃₂) points resembling those from the training data.\n",
    "'''\n",
    "G = nn.Sequential(      \n",
    "    nn.Linear(44, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 44)\n",
    ")\n",
    "\n",
    "'''\n",
    "#implementation of discriminator\n",
    "The discriminator is a model with a two-dimensional input and a one-dimensional output.\n",
    "It’ll receive a sample from the real data or from the generator \n",
    "and will provide the probability that the sample belongs to the real training data. \n",
    "'''\n",
    "\n",
    "D = nn.Sequential(                     \n",
    "    nn.Linear(44, 128),     \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3), #After the first, second, and third hidden layers, you use dropout to avoid overfitting.\n",
    "    nn.Linear(64, 1), #The output is composed of a single neuron with sigmoidal activation to represent a probability.\n",
    "    nn.Sigmoid()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before training the models, need to set up some parameters to use during training:\n",
    "BATCH_SIZE = 25\n",
    "lr = 0.001                   \n",
    "\n",
    "# optimizator\n",
    "opt_D = torch.optim.Adam(D.parameters(),lr=lr)\n",
    "opt_G = torch.optim.Adam(G.parameters(),lr=lr)\n",
    "num_epochs = 3000\n",
    "\n",
    "for step in range(num_epochs):\n",
    "    #Randomly select real samples of defined batch size with label 1\n",
    "    chosen_data = np.random.choice((X_forgenerate.shape[0]),size=(BATCH_SIZE),replace=False)\n",
    "    samples = X_forgenerate[chosen_data,:]\n",
    "    \n",
    "    # Generate fake samples using generators\n",
    "    G.zero_grad()        \n",
    "    generated_samples  = G(torch.randn(BATCH_SIZE, 44))                  \n",
    "    output_discriminator_generated = D(generated_samples) # feed the generator’s output into the discriminator \n",
    "    \n",
    "    # loss\n",
    "    G_loss = torch.mean(torch.log(1. - output_discriminator_generated))\n",
    "    G_loss.backward()\n",
    "    opt_G.step()\n",
    "    \n",
    "    # Training the discriminator\n",
    "    D.zero_grad()\n",
    "    output_discriminator = D(samples)\n",
    "    output_discriminator_generated = D(generated_samples.detach())\n",
    "    \n",
    "    # loss\n",
    "    D_loss = - torch.mean(torch.log(output_discriminator) + torch.log(1. - output_discriminator_generated))\n",
    "    D_loss.backward(retain_graph=True)\n",
    "    opt_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb9777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GAN fake data\n",
    "fake_data = G(torch.randn(n_generate,44)).detach().numpy()\n",
    "\n",
    "X_default = pd.DataFrame(np.concatenate([X_forgenerate,fake_data]))\n",
    "X_default['target'] = 1\n",
    "\n",
    "X_non_attrition = pd.DataFrame(X_non_attrition)\n",
    "X_non_attrition['target'] = 0\n",
    "train_data_gan = pd.concat([X_default,X_non_attrition])\n",
    "\n",
    "X_gan = train_data_gan.iloc[:,:-1]\n",
    "y_gan = train_data_gan.iloc[:,-1]\n",
    "\n",
    "print(X_gan.shape,y_gan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d1006",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f375f2",
   "metadata": {},
   "source": [
    "### 7. Testing SMOTE and GAN on Baseline Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm','Train Accuracy Mean','Test Accuracy', 'Recall']\n",
    "\n",
    "def runmodel(Xtrain, ytrain):\n",
    "    train_acc_results = []\n",
    "    test_acc_results = []\n",
    "    names = []\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=col)\n",
    "    \n",
    "    i = 0\n",
    "    # evaluate each model using cross-validation\n",
    "    for name, model in models:\n",
    "        kfold = KFold(n_splits=10)  # 10-fold cross-validation\n",
    "\n",
    "        # cv accuracy scoring\n",
    "        cv_acc_results = cross_val_score(model, Xtrain, ytrain, cv=kfold, scoring='accuracy')\n",
    "        train_acc_results.append(cv_acc_results)\n",
    "\n",
    "        #test accuracy scoring\n",
    "        model.fit(Xtrain,ytrain)\n",
    "                \n",
    "        pred = model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, pred)\n",
    "        test_acc_results.append(test_accuracy)\n",
    "        \n",
    "        #Confusion Matrix\n",
    "        cm = confusion_matrix(pred,y_test)\n",
    "        tn = cm[0][0]\n",
    "        fn = cm[1][0]\n",
    "        fp = cm[0][1]\n",
    "        tp = cm[1][1]\n",
    "        precision = round(tp/(tp+fp),4)\n",
    "        recall = round(tp/(tp+fn),4)\n",
    "        f1 = round((2*precision*recall)/(precision + recall),4)\n",
    "\n",
    "        names.append(name)\n",
    "        df_results.loc[i] = [name, round(cv_acc_results.mean()*100, 2), round(test_accuracy*100,2), round(recall*100,2)]\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of algorithms to consider and set performance measure\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(random_state=345)))\n",
    "models.append(('Random Forest', RandomForestClassifier(random_state=345)))\n",
    "models.append(('SVM', SVC(probability=True)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree Classifier',DecisionTreeClassifier(random_state=345)))\n",
    "models.append(('Gaussian NB', GaussianNB()))\n",
    "models.append(('Adaboost', AdaBoostClassifier(random_state=345)))\n",
    "models.append((\"Gradientboost\", GradientBoostingClassifier(random_state=345)))\n",
    "models.append((\"BaggingClassifier\", BaggingClassifier(random_state=345)))\n",
    "models.append((\"ExtremeGradientBoost\", XGBClassifier(random_state=345)))\n",
    "models.append((\"ExtraTreesClassifier\", ExtraTreesClassifier(random_state=345)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20534f4",
   "metadata": {},
   "source": [
    "###### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_result = runmodel(X_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51328f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_result.sort_values(by=['Test Accuracy', 'Recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f4510",
   "metadata": {},
   "source": [
    "###### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_result = runmodel(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be04d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_result.sort_values(by=['Test Accuracy', 'Recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951856ff",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67ffd1",
   "metadata": {},
   "source": [
    "### 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682eb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of algorithms to consider and set performance measure\n",
    "fs_models = []\n",
    "fs_models.append(('Logistic Regression', LogisticRegression(random_state=345)))\n",
    "fs_models.append(('Random Forest', RandomForestClassifier(random_state=345)))\n",
    "fs_models.append(('SVM', SVC(probability=True)))\n",
    "fs_models.append(('KNN', KNeighborsClassifier()))\n",
    "fs_models.append(('Decision Tree Classifier',DecisionTreeClassifier(random_state=345)))\n",
    "fs_models.append(('Gaussian NB', GaussianNB()))\n",
    "fs_models.append(('Adaboost', AdaBoostClassifier(random_state=345)))\n",
    "fs_models.append((\"Gradientboost\", GradientBoostingClassifier(random_state=345)))\n",
    "fs_models.append((\"BaggingClassifier\", BaggingClassifier(random_state=345)))\n",
    "fs_models.append((\"ExtremeGradientBoost\", XGBClassifier(random_state=345)))\n",
    "fs_models.append((\"ExtraTreesClassifier\", ExtraTreesClassifier(random_state=345)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c15dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_model(model, xtrain,ytrain, xtest):\n",
    "    \n",
    "    rfecv = RFECV(estimator=model, cv=StratifiedKFold(10, random_state=123, shuffle=True), scoring=\"accuracy\")\n",
    "    rfecv.fit(xtrain, ytrain)\n",
    "    \n",
    "    return xtrain.iloc[:, rfecv.support_], xtest.iloc[:, rfecv.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571283d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm','Train Accuracy Mean', 'Test Accuracy', \"Recall\"]\n",
    "\n",
    "def runfsmodel(X_train_importance, y_train_importance, x_test_importance):\n",
    "    train_acc_results = []\n",
    "    test_acc_results = []\n",
    "    names = []\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=col)\n",
    "    \n",
    "    i = 0\n",
    "    # evaluate each model using cross-validation\n",
    "    for name, model in fs_models:\n",
    "        kfold = KFold(n_splits=10)  # 10-fold cross-validation\n",
    "\n",
    "        # cv accuracy scoring\n",
    "        cv_acc_results = cross_val_score(model, X_train_importance, y_train_importance, cv=kfold, scoring='accuracy')\n",
    "        train_acc_results.append(cv_acc_results)\n",
    "\n",
    "        #test accuracy scoring\n",
    "        model.fit(X_train_importance,y_train_importance)\n",
    "        pred = model.predict(x_test_importance)\n",
    "        test_accuracy = accuracy_score(y_test, pred)\n",
    "        test_acc_results.append(test_accuracy)\n",
    "\n",
    "        #Confusion Matrix\n",
    "        cm = confusion_matrix(pred,y_test)\n",
    "        tn = cm[0][0]\n",
    "        fn = cm[1][0]\n",
    "        fp = cm[0][1]\n",
    "        tp = cm[1][1]\n",
    "        precision = round(tp/(tp+fp),4)\n",
    "        recall = round(tp/(tp+fn),4)\n",
    "        f1 = round((2*precision*recall)/(precision + recall),4)\n",
    "\n",
    "        names.append(name)\n",
    "        df_results.loc[i] = [name, round(cv_acc_results.mean()*100, 2), round(test_accuracy*100,2), round(recall*100,2)]\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e77c9",
   "metadata": {},
   "source": [
    "###### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cde245",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr_model = fs_models[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad415",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_fs = feature_selection_model(base_lr_model, X_train, y_train, X_test)\n",
    "LR_X_train_importance = LR_fs[0]\n",
    "LR_X_test_importance = LR_fs[1]\n",
    "\n",
    "print(f'Number of columns selected: {LR_X_train_importance.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_fs_result = runfsmodel(LR_X_train_importance, y_train, LR_X_test_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_fs_result.sort_values(by=['Test Accuracy', 'Recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc0657",
   "metadata": {},
   "source": [
    "###### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dt_model = fs_models[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_fs = feature_selection_model(base_dt_model, X_train, y_train, X_test)\n",
    "DT_X_train_importance = DT_fs[0]\n",
    "DT_X_test_importance = DT_fs[1]\n",
    "\n",
    "print(f'Number of columns selected: {DT_X_train_importance.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a22f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_fs_result = runfsmodel(DT_X_train_importance, y_train, DT_X_test_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_fs_result.sort_values(by=['Test Accuracy', 'Recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb930e4",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rf_model = fs_models[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_fs = feature_selection_model(base_rf_model, X_train, y_train, X_test)\n",
    "RF_X_train_importance = RF_fs[0]\n",
    "RF_X_test_importance = RF_fs[1]\n",
    "\n",
    "print(f'Number of columns selected: {RF_X_train_importance.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c008823",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_fs_result = runfsmodel(RF_X_train_importance, y_train, RF_X_test_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_fs_result.sort_values(by=['Test Accuracy', 'Recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdabda",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cecd2",
   "metadata": {},
   "source": [
    "## 9. Top 3 Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a136243",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"Accuracy_Score\",\"Recall\"]\n",
    "df_performance = pd.DataFrame(columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To evaluate performances of all the models\n",
    "def performance(name, p, ytest):\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(p,ytest),4)\n",
    "    \n",
    "    cm = confusion_matrix(p,ytest)\n",
    "    tn = cm[0][0]\n",
    "    fn = cm[1][0]\n",
    "    fp = cm[0][1]\n",
    "    tp = cm[1][1]\n",
    "    precision = round(tp/(tp+fp),4)\n",
    "    recall = round(tp/(tp+fn),4)\n",
    "    f1 = round((2*precision*recall)/(precision + recall),4)\n",
    "    \n",
    "    df_performance.loc[name] = [round(accuracy*100,2),round(recall*100,2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec9ba3",
   "metadata": {},
   "source": [
    "###### 1. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using randomsearchcv to determine what is the best parameters to use first\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param = {\n",
    "    'n_estimators':[100,200, 300,400,500,600],\n",
    "    'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20,30,40, 50,60, 100],\n",
    "    \"random_state\":[345]\n",
    "}\n",
    "rnd_search = RandomizedSearchCV(GradientBoostingClassifier(), param, n_iter =10, cv=9, random_state=200)\n",
    "rnd_search.fit(X_train,y_train)\n",
    "rnd_search.best_params_\n",
    "# rnd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters around that range plus,minus\n",
    "gb_params = {\n",
    "    'n_estimators':[300, 400,500],\n",
    "    'learning_rate': [0.2,0.3,0.4],\n",
    "    \"max_leaf_nodes\": [30,40,50],\n",
    "    \"random_state\":[345]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0459d00",
   "metadata": {},
   "source": [
    "performance gridserach on original train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df62d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_gs = GridSearchCV(GradientBoostingClassifier(),gb_params, scoring=make_scorer(accuracy_score),cv=5)\n",
    "gb_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bfc98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Best Params: {gb_gs.best_params_}')\n",
    "print(f'Best Estimator: {gb_gs.best_estimator_}')\n",
    "print(f'Best Score: {gb_gs.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = gb_gs.predict(X_test)\n",
    "\n",
    "performance(\"GradientBoosting_Tuned\",y_pred_gb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ad64c",
   "metadata": {},
   "source": [
    "###### 2. ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03234be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using randomsearchcv to determine what is the best parameters to use first\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param = {\n",
    "    \"max_depth\": [None,2,3,4,5,6,7,8,9,10,11,12,13],\n",
    "    'max_features':[\"sqrt\", \"log2\", None],\n",
    "    \"n_estimators\": [100,200,300,400,500,600],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"random_state\": [345]\n",
    "\n",
    "}\n",
    "rnd_search = RandomizedSearchCV(ExtraTreesClassifier(), param, n_iter =10, cv=9, random_state=200)\n",
    "rnd_search.fit(X_train,y_train)\n",
    "rnd_search.best_params_\n",
    "# rnd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b043324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoping down features for the max_depth and n_estimators, while leaving some options for max_features and criterions\n",
    "et_params = {\"max_depth\": [11,12,13],\n",
    "             'max_features':[\"sqrt\", \"log2\", None],\n",
    "             \"n_estimators\": [100,200,300],\n",
    "             \"criterion\": [\"gini\", \"entropy\", \"log_loss\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b424d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_gs = GridSearchCV(ExtraTreesClassifier(),et_params, scoring=make_scorer(accuracy_score),cv=5)\n",
    "et_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a280ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Params: {et_gs.best_params_}')\n",
    "print(f'Best Estimator: {et_gs.best_estimator_}')\n",
    "print(f'Best Score: {et_gs.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_et = et_gs.predict(X_test)\n",
    "\n",
    "performance(\"ExtraTreeClassifier_Tuned\",y_pred_et, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfc82c",
   "metadata": {},
   "source": [
    "###### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2637eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using randomsearchcv to determine what is the best parameters to use first\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param = {\n",
    "        'n_estimators':[100,200, 300, 400, 500,600,700],\n",
    "        'criterion':['gini','entropy'],\n",
    "        'max_depth':[None, 3,4,5,6,7,8,9],\n",
    "        'max_features':[\"sqrt\", \"log2\", None],\n",
    "        \"random_state\": [345]\n",
    "}\n",
    "rnd_search = RandomizedSearchCV(RandomForestClassifier(), param, n_iter =10, cv=9, random_state=200)\n",
    "rnd_search.fit(X_train,y_train)\n",
    "rnd_search.best_params_\n",
    "# rnd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params={\n",
    "        'n_estimators':[500,600, 700],\n",
    "        'criterion':['gini','entropy'],\n",
    "        'max_depth':[None,7,8,9],\n",
    "        'max_features':[\"sqrt\", \"log2\", None]\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71025b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs=GridSearchCV(RandomForestClassifier(),rf_params,scoring=make_scorer(accuracy_score),cv=5)\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af32c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Best Params: {rf_gs.best_params_}')\n",
    "print(f'Best Estimator: {rf_gs.best_estimator_}')\n",
    "print(f'Best Score: {rf_gs.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_gs.predict(X_test)\n",
    "\n",
    "performance(\"RandomForest_Tuned\",y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175c1a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_performance.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b20f0",
   "metadata": {},
   "source": [
    "### 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_index={'Gradientboost': 'Gradientboost_Baseline','ExtraTreesClassifier': 'ExtraTreesClassifier_Baseline','Random Forest': 'RandomForest_Baseline' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_baseline = smote_result.sort_values(by=['Test Accuracy', 'Recall'], ascending=False).iloc[:3].set_index(['Algorithm']).drop(['Train Accuracy Mean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_baseline.rename(columns={'Test Accuracy': 'Accuracy_Score'},index = rename_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e467794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df_performance, smote_baseline]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2a56e",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e40e91",
   "metadata": {},
   "source": [
    "### 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef081938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model w original dataset\n",
    "filename_gb_baseline = 'finalized_baseline_gbmodel.joblib'\n",
    "filename_et_baseline = 'finalized_baseline_etmodel.joblib'\n",
    "filename_rf_baseline = 'finalized_baseline_rfmodel.joblib'\n",
    "\n",
    "#tuned w original datset\n",
    "filename_gb = 'finalized_tuned_gbmodel.joblib'\n",
    "filename_et = 'finalized_tuned_etmodel.joblib'\n",
    "filename_rf = 'finalized_tuned_rfmodel.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(models[7][1], filename_gb_baseline)\n",
    "joblib.dump(models[10][1], filename_et_baseline)\n",
    "joblib.dump(models[1][1], filename_rf_baseline)\n",
    "\n",
    "joblib.dump(gb_gs.best_estimator_, filename_gb)\n",
    "joblib.dump(et_gs.best_estimator_, filename_et)\n",
    "joblib.dump(rf_gs.best_estimator_, filename_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312df5a",
   "metadata": {},
   "source": [
    "###### baseline w original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = joblib.load(filename_gb_baseline)\n",
    "et = joblib.load(filename_et_baseline)\n",
    "rf = joblib.load(filename_rf_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8adbb",
   "metadata": {},
   "source": [
    "##### tuned w original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb49c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_gs = joblib.load(filename_gb)\n",
    "et_gs = joblib.load(filename_et)\n",
    "rf_gs = joblib.load(filename_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea20c60",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1bce8",
   "metadata": {},
   "source": [
    "### 12. Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01dfa1",
   "metadata": {},
   "source": [
    "###### Voting Classifier with hard voting (Model trained with original SMOTE dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group / ensemble of best models\n",
    "estimator = []\n",
    "\n",
    "estimator.append(('gb', gb))\n",
    "estimator.append(('et', et))\n",
    "estimator.append(('rf', rf))\n",
    "estimator.append(('gb_gs', gb_gs))\n",
    "estimator.append(('et_gs', et_gs))\n",
    "estimator.append(('rf_gs', rf_gs))\n",
    "\n",
    "# Voting Classifier with hard voting\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "vot_hard.fit(X_train, y_train)\n",
    "y_pred = vot_hard.predict(X_test)\n",
    "\n",
    "# using accuracy_score metric to predict accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_pred,y_test)\n",
    "tn = cm[0][0]\n",
    "fn = cm[1][0]\n",
    "fp = cm[0][1]\n",
    "tp = cm[1][1]\n",
    "recall = round(tp/(tp+fn),4)\n",
    "print(f'Accuracy Score (Hard voting for original SMOTE dataset): {round(score*100,2)}%')\n",
    "print(f'Recall (Hard voting for original SMOTE dataset): {round(recall*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5dc99",
   "metadata": {},
   "source": [
    "###### Voting Classifier with soft voting (Model trained with original SMOTE dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abe05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "vot_soft.fit(X_train, y_train)\n",
    "y_pred = vot_soft.predict(X_test)\n",
    "# using accuracy_score\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_pred,y_test)\n",
    "tn = cm[0][0]\n",
    "fn = cm[1][0]\n",
    "fp = cm[0][1]\n",
    "tp = cm[1][1]\n",
    "recall = round(tp/(tp+fn),4)\n",
    "\n",
    "print(f'Accuracy Score (Soft voting for original SMOTE dataset): {round(score*100,2)}%')\n",
    "print(f'Recall (Soft voting for original SMOTE dataset): {round(recall*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90fb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "be9c48b7f3984ad819d14f6f9e9ad00f762af4dd73407fbdba0e95c704d060ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
